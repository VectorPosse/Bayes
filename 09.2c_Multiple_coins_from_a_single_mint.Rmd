---
title: "9.2(c): Multiple coins from a single mint"
output: html_notebook
---


## Introduction

This is the third of three models in Section 9.2 of Kruschke. It assumes a uniform prior on $\omega$ and allows the dependence of $\theta$ on $\omega$ to be determined by the data (with a gamma prior on $\kappa - 2$).


## Preliminaries

Load necessary packages:

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
library(bayesplot)
```

Set Stan to save compiled code.

```{r}
rstan_options(auto_write = TRUE)
```

Set Stan to use parallel processing where possible.

```{r}
options(mc.cores = parallel::detectCores())
```


## Data

Record the total number of successes (heads) from each coin.

```{r}
S <- 2 # 2 coins
N1 <- 15
N2 <- 5
N <- c(N1, N2) 
y1 <- 3 # 3 heads (of 15)
y2 <- 4 # 4 heads (of 5)
y <- c(y1, y2)
```

```{r}
stan_data <- list(S = S, N = N, y = y)
```


## Prior

### Simulation code

$A_{\omega}$ and $B_{\omega}$ are still constants. However, instead of a fixed value of $K$, we will use a parameter $\kappa$ that will allow the concentration of the beta distribution to be influenced by the data and a weak gamma prior. More specifically, we will use the parameter $\kappa - 2$: since the value of $\kappa$ needs to be greater than 2 for the beta distribution to make sense, but the gamma function starts at 0, we need to use a shifted version of $\kappa$. The real value of $\kappa$ that we care about will (somewhat perversely) have to be defined in the `transformed parameters` block with `kappa = kappa_minus_two + 2`.

Additionally, we need to specify constant shape and rate parameters ($S_{\kappa}$ and $R_{\kappa}$) for the gamma hyperprior. 

```{stan, output.var = "mcsmc_prior", cache = TRUE}
data {
    int<lower = 0> S;
    array[S] int<lower = 0> N;
    array[S] int<lower = 0> y;
}
transformed data {
    real<lower = 0> A_omega;
    real<lower = 0> B_omega;
    real<lower = 0> S_kappa;
    real<lower = 0> R_kappa;

    A_omega = 2;       // hyperprior parameters for omega
    B_omega = 2;
    S_kappa =  0.01;   // hyperprior parameters for kappa
    R_kappa =  0.01;
}
generated quantities {
    real<lower = 0, upper = 1> omega;
    real<lower = 0> kappa_minus_two;
    real<lower = 2> kappa;
    array[S] real<lower = 0, upper = 1> theta;
    real<lower = 0> a;
    real<lower = 0> b;
    array[S] int<lower = 0> y_sim;
    
    omega = beta_rng(A_omega, B_omega);  // mode
    kappa_minus_two = gamma_rng(S_kappa, R_kappa);
    kappa = kappa_minus_two + 2;         // concentration
    a = omega * (kappa - 2) + 1;
    b = (1 - omega) * (kappa - 2) + 1;
        
    for (s in 1:S) {
        theta[s] = beta_rng(a, b);  // probability of success
    }

    y_sim = binomial_rng(N, theta); // vectorized
}
```

```{r, cache = TRUE}
fit_mcsmc_prior <- sampling(mcsmc_prior,
                            data = stan_data,
                            chains = 1,
                            algorithm = "Fixed_param",
                            seed = 11111,
                            refresh = 0)
```

```{r}
samples_mcsmc_prior <- tidy_draws(fit_mcsmc_prior)
samples_mcsmc_prior
```

Use proportions of successes rather than counts:

```{r}
samples_mcsmc_prior <- samples_mcsmc_prior %>%
        mutate(`y_sim_prop[1]` = `y_sim[1]`/ N[1],
               `y_sim_prop[2]` = `y_sim[2]`/ N[2])
samples_mcsmc_prior
```


### Examine prior

```{r}
mcmc_hist(samples_mcsmc_prior,
          pars = "omega")
```

```{r}
mcmc_hist(samples_mcsmc_prior,
          pars = "kappa")
```

```{r}
mcmc_hist(samples_mcsmc_prior,
          pars = vars(starts_with("theta")))
```

```{r}
mcmc_pairs(fit_mcsmc_prior,
           pars = c("omega", "kappa"))
```

```{r}
mcmc_pairs(fit_mcsmc_prior,
           pars = vars("omega", starts_with("theta")))
```


```{r}
mcmc_pairs(fit_mcsmc_prior,
           pars = vars("kappa", starts_with("theta")))
```

### Prior predictive distribution

```{r}
y_sim_mcsmc_prior <- samples_mcsmc_prior %>%
    dplyr::select(starts_with("y_sim_prop")) %>%
    as.matrix()
```

```{r}
ppd_intervals(ypred = y_sim_mcsmc_prior)
```


## Model

```{stan, output.var = "mcsmc", cache = TRUE}
data {
    int<lower = 0> S;
    array[S] int<lower = 0> N;
    array[S] int<lower = 0> y;
}
transformed data {
    real<lower = 0> A_omega;  
    real<lower = 0> B_omega;
    real<lower = 0> S_kappa;
    real<lower = 0> R_kappa;

    A_omega = 2;       // hyperprior parameters for omega
    B_omega = 2;
    S_kappa =  0.01;   // hyperprior parameters for kappa
    R_kappa =  0.01;    
}
parameters {
    real<lower = 0, upper = 1> omega;
    real<lower = 0> kappa_minus_two;
    array[S] real<lower = 0, upper = 1> theta;
}
transformed parameters {
    real<lower = 2> kappa;
    real<lower = 0> a;
    real<lower = 0> b;
    
    kappa = kappa_minus_two + 2;     // concentration
    a = omega * (kappa - 2) + 1;
    b = (1 - omega) * (kappa - 2) + 1;
}
model {
    omega ~ beta(A_omega, B_omega);  // mode
    kappa_minus_two ~ gamma(S_kappa, R_kappa);
    theta ~ beta(a, b);              // prior prob of success
    y ~ binomial(N, theta);          // likelihood
}
generated quantities {
    array[S] int<lower = 0> y_sim;
    
    y_sim = binomial_rng(N, theta);
}
```

```{r, cache = TRUE}
fit_mcsmc <- sampling(mcsmc,
                      data = stan_data, 
                      seed = 11111,
                      refresh = 0)
```

When we put positive priors on variance parameters (like $\kappa$), we generally want to use priors that start off assuming the variance could be quite large, corresponding to values of $\kappa$ that are small. And while we generally want to assume values of $\kappa$ that are small, we also want a thick right tail that puts reasonable probability on larger values as well. These distributions, by necessity, are severely skewed and, therefore, can be difficult to sample from, as seen in the warning above.

"Divergent transitions" are a bit technical, but the idea is that the "physics" of the simulated trajectories gives us a way to detect when the leapfrog steps go "off the rails" and fail to correctly predict the path of the sampled steps. This tends to happen when there is a high degree of curvature in the probability surface we're exploring. These divergent proposals are rejected and Stan tells us how often this happen. Depending on the model, even a few divergences can fatally bias the results.

You may see advice online for how to "fix" divergent transitions. This may involve tweaking function parameters that control the step size, for example. Generally speaking, though, divergent transitions indicate a bad model, and you should find a better one. (The prior on $\kappa$ is almost exclusively to blame here.)

Just to show the results, we'll go ahead with the rest of our normal process.

```{r}
samples_mcsmc <- tidy_draws(fit_mcsmc)
samples_mcsmc
```

Again, we'll compute proportions:

```{r}
samples_mcsmc <- samples_mcsmc %>%
    mutate(`y_sim_prop[1]` = `y_sim[1]`/ N[1],
           `y_sim_prop[2]` = `y_sim[2]`/ N[2])
samples_mcsmc
```


## Model diagnostics

```{r}
stan_trace(fit_mcsmc,
           pars = c("omega", "kappa", "theta"))
```

Note that the traceplots for $\kappa$ are hard to see because they stay near 2 usually, but occasionally wander to much higher values.

```{r}
mcmc_acf(fit_mcsmc,
         pars = vars("omega", "kappa", starts_with("theta")))
```

```{r}
mcmc_rhat(rhat(fit_mcsmc))
```

```{r}
mcmc_neff(neff_ratio(fit_mcsmc))
```


## Model summary

```{r}
print(fit_mcsmc,
      pars = c("omega", "kappa", "theta"))
```


## Model visualization

```{r}
mcmc_areas(fit_mcsmc,
           pars = vars(starts_with(c("omega", "theta"))))
```

```{r}
mcmc_hist(fit_mcsmc,
          pars = "kappa")
```


```{r}
pairs(fit_mcsmc,
      pars = c("omega", "theta"))
```

(The red dots indicate the divergent transitions.)


## Posterior predictive check

```{r}
y_sim_mcsmc <- samples_mcsmc %>%
    dplyr::select(starts_with("y_sim_prop")) %>%
    as.matrix()
```

```{r}
ppc_intervals(y = y / N,
              yrep = y_sim_mcsmc)
```

The prior is so diffuse that the posterior is basically just following the data, but with a lot of possible variability.

Coin 1:

```{r}
ppc_stat(y = y[1] / N[1],
         yrep = as.matrix(y_sim_mcsmc[ , 1]))
```

Coin 2:

```{r}
ppc_stat(y = y[2] / N[2],
         yrep = as.matrix(y_sim_mcsmc[ , 2]))
```
