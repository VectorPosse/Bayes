---
title: "Illustrating Bayes' Theorem"
author: "Sean Raleigh"
output: html_notebook
---


## Preliminaries

Here we load necessary libraries and define the `bayes_plot` function.

```{r}
# Load packages
library(ggplot2) # for plotting
library(triangle) # to use the triangular distribution

# bayes_plot takes a prior function and a likelihood function on a range of 
# values, calculates the posterior, and then plots all three functions.
bayes_plot <- function(prior, likelihood, from, to) {
    likelihood_norm <- integrate(likelihood, from, to)
    likelihood_scaled <- function(theta) {
        likelihood(theta)/likelihood_norm$value
    }
    posterior_num <- function(theta) {
        prior(theta) * likelihood(theta)
        }
    posterior_denom <- integrate(posterior_num, from, to)
    posterior <- function(theta) {
        posterior_num(theta)/posterior_denom$value
        }
    
    posterior_plot <- ggplot(NULL, aes(x = x, color = col, linetype = col)) +
        stat_function(data = data.frame(x = c(from, to), col = factor(1)),
                      fun = prior) +
        stat_function(data = data.frame(x = c(from, to), col = factor(2)),
                      fun = posterior) +
        stat_function(data = data.frame(x = c(from, to), col = factor(3)),
                      fun = likelihood_scaled) +
        theme_bw() +
        theme(panel.grid = element_blank()) +
        ggtitle("Prior, Posterior, and Scaled Likelihood") +
        xlab(expression(theta)) +
        ylab(NULL) +
        scale_colour_manual(name = "Function",
                            values = c("blue", "black", "red"),
                            labels = c("Prior",
                                       "Posterior",
                                       "Scaled Likelihood")) +
        scale_linetype_manual(name = "Function",
                              values = c("dotted", "solid", "dashed"),
                              labels = c("Prior",
                                         "Posterior",
                                         "Scaled Likelihood"))
    posterior_plot
}
```


## Binomial likelihood

Suppose we have binomial data. In 18 trials, we observe 12 successes. The likelihood function is expressed as follows:

$$ p(y = 12 \mid \theta) \propto \theta^{12} (1 - \theta)^{6} $$


```{r}
likelihood1 <- function(theta) { dbinom(x = 12, size = 18, prob = theta) }

likelihood1_plot <- ggplot(data.frame(x = c(0,1)), aes(x = x)) +
    stat_function(fun = likelihood1) +
    theme_bw() +
    theme(panel.grid = element_blank()) +
    ggtitle("Binomial Likelihood") +
    xlab(expression(theta)) +
    ylab(NULL)
likelihood1_plot
```


## Choosing a uniform prior

Assume a uniform prior:

$$p(\theta) = 1.$$

Then the posterior is

<!-- If knitting to HTML, the following align environment needs to
be surrounded with double dollar signs.
If knitting to PDF, then remove the dollar signs. -->

$$
\begin{align*}
    p(\theta \mid y)
        &\propto    p(\theta) p(y \mid \theta)  \\
        &=          p(y \mid \theta).
\end{align*}
$$

In this case, the posterior has exactly the same shape as the likelihood. The likelihood function is not a probability density function---its area is not one. But we can scale the likelihood function by dividing it by its area, and now it will have area one. One can see in the graph that with a uniform prior, the posterior is just the scaled likelihood function. With no prior information, the data is our best guess for the posterior.

```{r}
prior1 <- function(theta) { 1 }
bayes_plot(prior1, likelihood1, 0, 1)
```


## Choosing a prior far from the data

Suppose we now choose a prior that is relatively far from the data, say, a normal distibution centered at 0.3 with standard deviation 0.1:

$$\theta \sim N(0.3, 0.1).$$

The posterior is a compromise between the prior and the data, so if the prior and data are far apart, the posterior will end up being in between them somewhere.

```{r}
prior2 <- function(theta) { dnorm(theta, mean = 0.3, sd = 0.1) }
bayes_plot(prior2, likelihood1, 0, 1)
```


## Choosing a prior close to the data

What about a prior that is close to the data? Something like

$$\theta \sim N(0.7, 0.1).$$

In this case, the data and the prior reinforce each other.

```{r}
prior3 <- function(theta) { dnorm(theta, mean = 0.7, sd = 0.1) }
bayes_plot(prior3, likelihood1, 0, 1)
```


## Changing the shape of the prior

Of course, any kind of distribution can be a prior. What about a triangular distribution? Note that even with the peak at 0.5, there is still a lot of prior mass spread from 0 to 1. This triangular prior is quite diffuse, so the data will have more weight in determining the posterior.

```{r}
prior4 <- function(theta) { dtriangle(theta, a = 0, b = 1) }
bayes_plot(prior4, likelihood1, 0, 1)
```


## Changing the data

Now let's go back through the previous cases, but make the data much weaker. Instead of 12 successes in 18 trials, suppose we only observe 2 successes in 3 trials. Notice that the proportion of successes hasn't changed here, just the sample size. Observe that in each case, the posterior is much closer to the prior. There just isn't enough data for us to revise our prior belief radically.

```{r}
likelihood2 <- function(theta) { dbinom(x = 2, size = 3, prob = theta) } 
bayes_plot(prior1, likelihood2, 0, 1)
```

```{r}
bayes_plot(prior2, likelihood2, 0, 1)
```

```{r}
bayes_plot(prior3, likelihood2, 0, 1)
```

```{r}
bayes_plot(prior4, likelihood2, 0, 1)
```
