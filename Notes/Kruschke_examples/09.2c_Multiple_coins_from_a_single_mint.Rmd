---
title: "9.2(c): Multiple coins from a single mint"
output: html_notebook
---


## Introduction

This is the third of three models in Section 9.2 of Kruschke. It assumes a uniform prior on $\omega$ and allows the dependence of $\theta$ on $\omega$ to be determined by the data (with a gamma prior on $\kappa - 2$).


## Preliminaries

Load necessary packages:

```{r, message = FALSE}
library(rstan)
library(shinystan)
library(bayesplot)
```

Set Stan to save compiled code.

```{r}
rstan_options(auto_write = TRUE)
```

Set Stan to use parallel processing where possible.

```{r}
options(mc.cores = parallel::detectCores())
```


## Data

```{r}
S <- 2 # 2 coins
N1 <- 15 # Number of flips for coin 1
N2 <- 5  # Number of flips for coin 2
N <- c(N1, N2) 
y1 <- 3 # 3 heads (of 15)
y2 <- 4 # 4 heads (of 5)
y <- c(y1, y2)
stan_data <- list(S = S, N = N, y = y)
```


## Stan code

$A_{\omega}$ and $B_{\omega}$ are still constants. However, instead of a fixed value of $K$, we will use a parameter $\kappa$ that will allow the concentration of the beta distribution to be influenced by the data and a weak gamma prior. More specifically, we will use the parameter $\kappa - 2$: since the value of $\kappa$ needs to be greater than 2 for the beta distribution to make sense, but the gamma function starts at 0, we need to use a shifted version of $\kappa$. The real value of $\kappa$ that we care about will (somewhat perversely) have to be defined in the `transformed parameters` block with `kappa = kappa_minus_two + 2`.

Additionally, we need to specify constant shape and rate parameters ($S_{\kappa}$ and $R_{\kappa}$) for the gamma hyperprior. 


```{stan, output.var = "mcsmc", cache = TRUE}
data {
    int<lower = 0> S;
    int<lower = 0> N[S];
    int<lower = 0> y[S];
}
transformed data {
    real<lower = 0> A_omega;
    real<lower = 0> B_omega;
    real<lower = 0> S_kappa;
    real<lower = 0> R_kappa;

    A_omega = 2;
    B_omega = 2;
    S_kappa =  0.01;
    R_kappa =  0.01;
}
parameters {
    real<lower = 0, upper = 1> omega;
    real<lower = 0> kappa_minus_two;
    real<lower = 0, upper = 1> theta[S];
}
transformed parameters {
    real<lower = 2> kappa;
    real<lower = 0> a;
    real<lower = 0> b;
    
    kappa = kappa_minus_two + 2;
    a = omega * (kappa - 2) + 1;
    b = (1 - omega) * (kappa - 2) + 1;
}
model {
    omega ~ beta(A_omega, B_omega);
    kappa_minus_two ~ gamma(S_kappa, R_kappa);
    theta ~ beta(a, b);
    y ~ binomial(N, theta);
}
generated quantities {
    int<lower = 0> y_rep[S];
    
    for (s in 1:S) {
        y_rep[s] = binomial_rng(N[s], theta[s]);
    }
}
```


## Sampling from the model

When we put positive priors on variance parameters (like $\kappa$), we generally want to use priors that start off assuming the variance could be quite large, corresponding to values of $\kappa$ that are small. And while we generally want to assume values of $\kappa$ that are small, we also want a thick right tail that puts reasonable probability on larger values as well. These distributions, by necessity, are severely skewed and, therefore, can be difficult to sample from.

Take a look at the warning below this first attempt:

```{r}
set.seed(11111)
fit_mcsmc <- sampling(mcsmc, data = stan_data, refresh = 0)
```

"Divergent transitions" are a bit technical, but the idea is that the "physics" of the simulated trajectories gives us a way to detect when the leapfrog steps go "off the rails" and fail to correctly predict the path of the sampled steps. This tends to happen when there is a high degree of curvature in the probability surface we're exploring. These divergent proposals are rejected and Stan tells us how often this happen. Depending on the model, even a few divergences can fatally bias the results.

One way to fix the issue is to reduce the step size. By taking smaller steps, the algorithm can stay closer to the "true" trajectory it's trying to simulate. The way to take smaller steps is to increase the `adapt_delta` argument of the `sampling` function. (It has to be some value between 0 and 1; the default in 0.8.)

```{r}
set.seed(11111)
fit_mcsmc <- sampling(mcsmc, data = stan_data, refresh = 0,
                      control = list(adapt_delta = 0.99))
```

This helped a bit. We're also getting warnings about "treedepth". These are less serious because it just means that each simulation is taking a long time to execute. We can avoid these warnings by increasing the value of `max_treedepth`.

So let's do this one final time, making `adapt_delta` even larger and tweaking the `max_treedepth`.

```{r}
set.seed(11111)
fit_mcsmc <- sampling(mcsmc, data = stan_data, refresh = 0,
                      control = list(adapt_delta = 0.999,
                                     max_treedepth = 15))
```

We'll want to inspect those divergent transitions graphically to make sure they aren't concentrating in one area of the model. (This is easiest to do in ShinyStan.)


## Diagnosing the model

```{r}
plot(fit_mcsmc, plotfun = "ac",
     pars = c("omega", "theta", "kappa"))
```

```{r}
plot(fit_mcsmc, plotfun = "trace",
     pars = c("omega", "theta", "kappa"))
```

Note that the traceplots for $\kappa$ are hard to see because they stay near 2 usually, but occasionally wander to much higher values.


## Summarizing the model

```{r}
fit_mcsmc
```


## Visualizing the model

```{r}
pairs(fit_mcsmc, pars = c("omega", "theta"))
```

```{r}
pairs(fit_mcsmc, pars = c("omega", "kappa"))
```

```{r}
plot(fit_mcsmc, pars = c("omega", "theta"))
```


```{r}
plot(fit_mcsmc, pars = "kappa")
```


## Examining the prior

```{stan, output.var = "mcsmc_prior", cache = TRUE}
data {
    int<lower = 0> S;
    int<lower = 0> N[S];
    int<lower = 0> y[S];
}
transformed data {
    real<lower = 0> A_omega;
    real<lower = 0> B_omega;
    real<lower = 0> S_kappa;
    real<lower = 0> R_kappa;

    A_omega = 2;
    B_omega = 2;
    S_kappa =  0.01;
    R_kappa =  0.01;
}
parameters {
    real<lower = 0, upper = 1> omega;
    real<lower = 0> kappa_minus_two;
    real<lower = 0, upper = 1> theta[S];
}
transformed parameters {
    real<lower = 2> kappa;
    real<lower = 0> a;
    real<lower = 0> b;
    
    kappa = kappa_minus_two + 2;
    a = omega * (kappa - 2) + 1;
    b = (1 - omega) * (kappa - 2) + 1;
}
model {
    omega ~ beta(A_omega, B_omega);
    kappa_minus_two ~ gamma(S_kappa, R_kappa);
    theta ~ beta(a, b);
//    y ~ binomial(N, theta);
}
```

```{r}
set.seed(11111)
fit_mcsmc_prior <- sampling(mcsmc_prior, data = stan_data, refresh = 0,
                            control = list(adapt_delta = 0.999,
                                           max_treedepth = 15))
```

```{r}
fit_mcsmc_prior
```

```{r}
pairs(fit_mcsmc_prior, pars = c("omega", "theta"))
```

```{r}
pairs(fit_mcsmc_prior, pars = c("omega", "kappa"))
```


```{r}
plot(fit_mcsmc_prior, pars = c("omega", "theta"))
```

```{r}
plot(fit_mcsmc_prior, pars = "kappa")
```

## Posterior predictive check

Extract predicted values:

```{r}
samples_mcsmc <- extract(fit_mcsmc)
y_rep <- samples_mcsmc$y_rep
```

Graph values of $y$ against summaries of $y_{rep}$.

(The `ppc_stat` function requires a matrix as input, so we have to convert the single columns of values in `y_rep[, 1]` to a 4000 by 1 matrix.)

```{r}
ppc_stat(y[1], as.matrix(y_rep[ , 1]))
```

```{r}
ppc_stat(y[2], as.matrix(y_rep[ , 2]))
```

The $x$ value in the graph below simply identifies the two coins.

```{r}
ppc_intervals(y, y_rep)
```


## ShinyStan

Run the following code from the Console:

```
launch_shinystan(fit_mcsmc)
```